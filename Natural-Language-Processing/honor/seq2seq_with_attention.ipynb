{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datasets\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83097/83097 [00:04<00:00, 20440.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('can we make this quick roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad again',\n",
       "  'well i thought wed start with pronunciation if thats okay with you'),\n",
       " ('well i thought wed start with pronunciation if thats okay with you',\n",
       "  'not the hacking and gagging and spitting part please'),\n",
       " ('not the hacking and gagging and spitting part please',\n",
       "  'okay then how bout we try out some french cuisine saturday night')]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = os.path.join(\"data\", \"cornell\")\n",
    "data = datasets.readCornellData(dataset_path, max_len=100000)\n",
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[sentences[0].split(), sentences[1].split()]for sentences in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [sentences for sentences in data if len(sentences[0]) >= 3]\n",
    "data = [sentences for sentences in data if len(sentences[0]) <= 20]\n",
    "data = [sentences for sentences in data if len(sentences[1]) >= 3]\n",
    "data = [sentences for sentences in data if len(sentences[1]) <= 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort(key=lambda x:len(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = [\n",
    "    [\"where are you from\", \"china\"],\n",
    "    [\"how are you\", \"im fine\"],\n",
    "    [\"how do you do\", \"im good\"],\n",
    "    [\"what is your hobby\", \"soccer\"],\n",
    "    [\"what is your hobby\", \"movie\"],\n",
    "    [\"do you love me\", \"of course\"],\n",
    "    [\"what is your name\", \"bot\"]\n",
    "]\n",
    "new_data = [[sentences[0].split(), sentences[1].split()]for sentences in new_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = new_data + data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'real', 'you'], ['like', 'my', 'fear', 'of', 'wearing', 'pastels']]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "word_count = defaultdict(int)\n",
    "for sentences in data:\n",
    "    for word in sentences[0]:\n",
    "        word_count[word] += 1\n",
    "    for word in sentences[1]:\n",
    "        word_count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = set([word for word in word_count if word_count[word]>=5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_token = \"[UKN]\"\n",
    "start_token = \"[START]\"\n",
    "end_token = \"[END]\"\n",
    "pad_token = \"[PAD]\"\n",
    "word_set.add(unknown_token)\n",
    "word_set.add(start_token)\n",
    "word_set.add(end_token)\n",
    "word_set.add(pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {}\n",
    "idx_to_word = [None] * len(word_set)\n",
    "index = 0\n",
    "for word in word_set:\n",
    "    word_to_idx[word] = index\n",
    "    idx_to_word[index] = word\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_idx = word_to_idx[unknown_token]\n",
    "start_idx = word_to_idx[start_token]\n",
    "end_idx = word_to_idx[end_token]\n",
    "pad_idx = word_to_idx[pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentences in data:\n",
    "    sentences[1].append(end_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengths = [len(sentences[0]) for sentences in data]\n",
    "ground_truth_lengths = [len(sentences[1]) for sentences in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add padding\n",
    "max_input_lengths = max(input_lengths)\n",
    "max_ground_truth_lengths = max(ground_truth_lengths)\n",
    "input_sentences = []\n",
    "ground_truth_sentences = []\n",
    "for sentences in data:\n",
    "    input_sentences.append(sentences[0] + [pad_token]*(max_input_lengths-len(sentences[0])))\n",
    "    ground_truth_sentences.append(sentences[1] + [pad_token]*(max_ground_truth_lengths-len(sentences[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences_idx = [[word_to_idx[word] if word in word_to_idx else unknown_idx for word in sentence] for sentence in input_sentences]\n",
    "ground_truth_sentences_idx = [[word_to_idx[word] if word in word_to_idx else unknown_idx for word in sentence] for sentence in ground_truth_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences_idx = np.array(input_sentences_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_sentences_idx = np.array(ground_truth_sentences_idx)\n",
    "input_lengths = np.array(input_lengths)\n",
    "ground_truth_lengths = np.array(ground_truth_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(batch_size,\n",
    "                    input_sentences_idx,\n",
    "                    ground_truth_sentences_idx,\n",
    "                    input_lengths,\n",
    "                    ground_truth_lengths):\n",
    "    index = 0\n",
    "    while index < len(input_sentences_idx):\n",
    "        batch_input_length = input_lengths[index:index+batch_size]\n",
    "        batch_input_sentences_idx = input_sentences_idx[index:index+batch_size, :batch_input_length.max()]\n",
    "        batch_ground_truth_length = ground_truth_lengths[index:index+batch_size]\n",
    "        batch_ground_truth_sentences_idx = ground_truth_sentences_idx[index:index+batch_size, :batch_ground_truth_length.max()]\n",
    "        yield (batch_input_sentences_idx, batch_input_length,\n",
    "              batch_ground_truth_sentences_idx, batch_ground_truth_length)\n",
    "        index += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_set)\n",
    "num_units = 128\n",
    "embedding_size = 100\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prepare(text):\n",
    "    \"\"\"Performs tokenization and simple preprocessing.\"\"\"\n",
    "    \n",
    "    replace_by_space_re = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "    good_symbols_re = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "    text = text.lower()\n",
    "    text = replace_by_space_re.sub(' ', text)\n",
    "    text = good_symbols_re.sub('', text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    def __init__(self):\n",
    "        self.declare_placeholders()\n",
    "        self.build_input_encoder()\n",
    "        self.build_decoder()\n",
    "        self.define_loss_and_train()\n",
    "    \n",
    "    def declare_placeholders(self):\n",
    "        \"\"\"Specifies placeholders for the model.\"\"\"\n",
    "        # Placeholders for input and its actual lengths.\n",
    "        self.input_batch = tf.placeholder(shape=(None, None), dtype=tf.int32, name='input_batch')\n",
    "        self.input_batch_lengths = tf.placeholder(shape=(None, ), dtype=tf.int32, name='input_batch_lengths')\n",
    "\n",
    "        # Placeholders for groundtruth and its actual lengths.\n",
    "        self.ground_truth = tf.placeholder(shape=(None, None), dtype=tf.int32, name='ground_truth')\n",
    "        self.ground_truth_lengths = tf.placeholder(shape=(None, ), dtype=tf.int32, name='ground_truth_lengths')\n",
    "\n",
    "        self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
    "        self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "\n",
    "    def build_input_encoder(self):\n",
    "        with tf.variable_scope('input_encoder') as input_encoder_scope:\n",
    "            random_initializer = tf.random_uniform((vocab_size, embedding_size), -1.0, 1.0)\n",
    "            self.embeddings = tf.Variable(initial_value=random_initializer, name='embeddings', dtype=tf.float32) \n",
    "\n",
    "            # Perform embeddings lookup for self.input_batch. \n",
    "            self.input_batch_embedded = tf.nn.embedding_lookup(self.embeddings, self.input_batch)\n",
    "            # Create encoder cells\n",
    "            rnn_layers = []\n",
    "            for i in range(num_encoder_layers-1):\n",
    "                with tf.variable_scope('input_encoder_rnn_layer' + str(i + 1)) as scope:\n",
    "                    cell = tf.nn.rnn_cell.GRUCell(num_units, activation=tf.nn.relu)\n",
    "                    cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=self.dropout_ph, dtype=tf.float32)\n",
    "                    rnn_layers.append(cell)\n",
    "            with tf.variable_scope('input_encoder_rnn_layer' + str(num_encoder_layers)) as scope:\n",
    "                cell = tf.nn.rnn_cell.GRUCell(num_units)\n",
    "                cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=self.dropout_ph, dtype=tf.float32)\n",
    "                rnn_layers.append(cell)\n",
    "            encoder_cell = tf.contrib.rnn.MultiRNNCell(rnn_layers)\n",
    "            self.input_encoder_outputs, self.final_input_encoder_state = tf.nn.dynamic_rnn(\n",
    "                encoder_cell,\n",
    "                self.input_batch_embedded,\n",
    "                sequence_length=self.input_batch_lengths,\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            self.final_input_encoder_state = self.final_input_encoder_state[-1]\n",
    "\n",
    "    def build_decoder(self):\n",
    "        batch_size = tf.shape(self.input_batch)[0]\n",
    "        start_tokens = tf.fill([batch_size], start_idx)\n",
    "        ground_truth_as_input = tf.concat([tf.expand_dims(start_tokens, 1), self.ground_truth], 1)\n",
    "        self.ground_truth_embedded = tf.nn.embedding_lookup(\n",
    "            self.embeddings, ground_truth_as_input)\n",
    "        train_helper = tf.contrib.seq2seq.TrainingHelper(self.ground_truth_embedded,\n",
    "                                                         self.ground_truth_lengths)\n",
    "        infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(self.embeddings, start_tokens, end_idx)\n",
    "\n",
    "        def decode(helper, scope, reuse=None):\n",
    "            \"\"\"Creates decoder and return the results of the decoding with a given helper.\"\"\"\n",
    "            with tf.variable_scope(scope, reuse=reuse):\n",
    "                # Create GRUCell with dropout. Do not forget to set the reuse flag properly.\n",
    "                rnn_layers = []\n",
    "                for i in range(num_decoder_layers-1):\n",
    "                    with tf.variable_scope('decoder_rnn_layer' + str(i + 1)) as scope:\n",
    "                        decoder_cell = tf.contrib.rnn.GRUCell(num_units=num_units, reuse=reuse, activation=tf.nn.tanh)\n",
    "                        decoder_cell = tf.contrib.rnn.DropoutWrapper(decoder_cell, input_keep_prob=self.dropout_ph)\n",
    "                        rnn_layers.append(decoder_cell)\n",
    "                with tf.variable_scope('decoder_rnn_layer' + str(num_decoder_layers)) as scope:\n",
    "                    decoder_cell = tf.contrib.rnn.GRUCell(num_units=num_units, reuse=reuse)\n",
    "                    decoder_cell = tf.contrib.rnn.DropoutWrapper(decoder_cell, input_keep_prob=self.dropout_ph)\n",
    "                    rnn_layers.append(decoder_cell)\n",
    "                decoder_cell = tf.contrib.rnn.MultiRNNCell(rnn_layers)\n",
    "                # Create attention\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    num_units=num_units, memory=tf.split(self.input_encoder_outputs, num_or_size_splits=2, axis=-1)[0],\n",
    "                    memory_sequence_length=self.input_batch_lengths)\n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                    decoder_cell, attention_mechanism, attention_layer_size=num_units)\n",
    "                # Create a projection wrapper.\n",
    "                decoder_cell = tf.contrib.rnn.OutputProjectionWrapper(decoder_cell, vocab_size, reuse=reuse)\n",
    "                # Create BasicDecoder, pass the defined cell, a helper, and initial state.\n",
    "                # The initial state should be equal to the final state of the encoder!\n",
    "                decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_cell, helper=helper, initial_state=decoder_cell.zero_state(\n",
    "                    dtype=tf.float32, batch_size=batch_size))\n",
    "\n",
    "                # The first returning argument of dynamic_decode contains two fields:\n",
    "                #   rnn_output (predicted logits)\n",
    "                #   sample_id (predictions)\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=decoder, maximum_iterations=tf.reduce_max(self.ground_truth_lengths), \n",
    "                                                                  output_time_major=False, impute_finished=True)\n",
    "\n",
    "                return outputs\n",
    "\n",
    "        self.train_outputs = decode(train_helper, 'decode')\n",
    "        self.infer_outputs = decode(infer_helper, 'decode', reuse=True)\n",
    "        self.train_predictions = self.train_outputs.sample_id\n",
    "        self.infer_predictions = self.infer_outputs.sample_id\n",
    "\n",
    "    def define_loss_and_train(self):\n",
    "        weights = tf.cast(tf.sequence_mask(self.ground_truth_lengths), dtype=tf.float32)\n",
    "        self.loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            self.train_outputs.rnn_output,\n",
    "            self.ground_truth,\n",
    "            weights\n",
    "        )\n",
    "        self.train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=self.loss,\n",
    "            optimizer='Adam',\n",
    "            learning_rate=self.learning_rate_ph,\n",
    "            clip_gradients=1.0,\n",
    "            global_step=tf.train.get_global_step()\n",
    "        )\n",
    "\n",
    "    def train_on_batch(self, session, X, X_seq_len, Y, Y_seq_len, learning_rate, dropout_keep_probability):\n",
    "        feed_dict = {\n",
    "            self.input_batch: X,\n",
    "            self.input_batch_lengths: X_seq_len,\n",
    "            self.ground_truth: Y,\n",
    "            self.ground_truth_lengths: Y_seq_len,\n",
    "            self.learning_rate_ph: learning_rate,\n",
    "            self.dropout_ph: dropout_keep_probability\n",
    "        }\n",
    "        loss, _ = session.run([\n",
    "            self.loss,\n",
    "            self.train_op], feed_dict=feed_dict)\n",
    "        return loss\n",
    "    \n",
    "    def get_reply(self, session, input_sentence):\n",
    "        input_sentence = text_prepare(input_sentence)\n",
    "        X = [[word_to_idx[word] if word in word_to_idx else unknown_idx for word in input_sentence]]\n",
    "        X = np.array(X)\n",
    "        feed_dict = {\n",
    "            self.input_batch: X,\n",
    "            self.input_batch_lengths: np.array([len(input_sentence)]),\n",
    "            self.ground_truth_lengths: np.array([15])\n",
    "        }\n",
    "        pred = session.run([self.infer_predictions], feed_dict=feed_dict)\n",
    "        return \" \".join([idx_to_word[index] for index in pred[0][0][:-1]])\n",
    "\n",
    "    def train(self, session, epochs, batch_size, input_sentences_idx, ground_truth_sentences_idx, input_lengths, ground_truth_lengths, learning_rate, dropout_keep_probability):\n",
    "        for i in range(epochs):\n",
    "            batch_num = 1\n",
    "            for (batch_input_sentences_idx,\n",
    "                 batch_input_length,\n",
    "                 batch_ground_truth_sentences_idx,\n",
    "                 batch_ground_truth_length) in batch_generator(\n",
    "                batch_size, input_sentences_idx, ground_truth_sentences_idx,\n",
    "                input_lengths, ground_truth_lengths):\n",
    "                loss = self.train_on_batch(\n",
    "                    session,\n",
    "                    batch_input_sentences_idx,\n",
    "                    batch_input_length,\n",
    "                    batch_ground_truth_sentences_idx,\n",
    "                    batch_ground_truth_length,\n",
    "                    learning_rate,\n",
    "                    dropout_keep_probability\n",
    "                )\n",
    "                print(\"Epoch {i}, batch {batch}, loss = {loss}\".format(i=i+1, batch=batch_num, loss=loss))\n",
    "                batch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = ChatBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 1, loss = 6.808646202087402\n",
      "Epoch 1, batch 2, loss = 6.779938697814941\n",
      "Epoch 1, batch 3, loss = 6.709688663482666\n",
      "Epoch 1, batch 4, loss = 6.65559196472168\n",
      "Epoch 1, batch 5, loss = 6.599919319152832\n",
      "Epoch 1, batch 6, loss = 6.496409893035889\n",
      "Epoch 1, batch 7, loss = 6.345330238342285\n",
      "Epoch 1, batch 8, loss = 6.384521484375\n",
      "Epoch 1, batch 9, loss = 6.314704418182373\n",
      "Epoch 1, batch 10, loss = 6.217668056488037\n"
     ]
    }
   ],
   "source": [
    "chatbot.train(session,\n",
    "              1, 1024,\n",
    "              input_sentences_idx[:10240],\n",
    "              ground_truth_sentences_idx[:10240],\n",
    "              input_lengths[:10240],\n",
    "              ground_truth_lengths[:10240],\n",
    "              5e-4,\n",
    "              0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i i i [END]'"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.get_reply(session, \"how are you\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
