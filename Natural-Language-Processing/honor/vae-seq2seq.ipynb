{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/maxpoon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datasets\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(\"data\", \"cornell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83097/83097 [00:03<00:00, 21042.81it/s]\n"
     ]
    }
   ],
   "source": [
    "data = datasets.readCornellData(dataset_path, max_len=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('can we make this quick roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad again',\n",
       "  'well i thought wed start with pronunciation if thats okay with you'),\n",
       " ('well i thought wed start with pronunciation if thats okay with you',\n",
       "  'not the hacking and gagging and spitting part please'),\n",
       " ('not the hacking and gagging and spitting part please',\n",
       "  'okay then how bout we try out some french cuisine saturday night')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove short sentences\n",
    "data = [sentences for sentences in data if len(sentences[0].split()) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cesc ma tete this is my head', 'right see youre ready for the quiz')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187012"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('that should put us ahead of the criminals', 'ill work on it')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[sentences[0].split(), sentences[1].split()]for sentences in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['can',\n",
       "  'we',\n",
       "  'make',\n",
       "  'this',\n",
       "  'quick',\n",
       "  'roxanne',\n",
       "  'korrine',\n",
       "  'and',\n",
       "  'andrew',\n",
       "  'barrett',\n",
       "  'are',\n",
       "  'having',\n",
       "  'an',\n",
       "  'incredibly',\n",
       "  'horrendous',\n",
       "  'public',\n",
       "  'break',\n",
       "  'up',\n",
       "  'on',\n",
       "  'the',\n",
       "  'quad',\n",
       "  'again'],\n",
       " ['well',\n",
       "  'i',\n",
       "  'thought',\n",
       "  'wed',\n",
       "  'start',\n",
       "  'with',\n",
       "  'pronunciation',\n",
       "  'if',\n",
       "  'thats',\n",
       "  'okay',\n",
       "  'with',\n",
       "  'you']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [sentences for sentences in data if len(sentences[0]) >= 3]\n",
    "data = [sentences for sentences in data if len(sentences[0]) <= 20]\n",
    "data = [sentences for sentences in data if len(sentences[1]) >= 3]\n",
    "data = [sentences for sentences in data if len(sentences[1]) <= 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort(key=lambda x:len(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['what', 'good', 'stuff'], ['the', 'real', 'you']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115832"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "word_count = defaultdict(int)\n",
    "for sentences in data:\n",
    "    for word in sentences[0]:\n",
    "        word_count[word] += 1\n",
    "    for word in sentences[1]:\n",
    "        word_count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = set([word for word in word_count if word_count[word]>=5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_token = \"[UKN]\"\n",
    "start_token = \"[START]\"\n",
    "end_token = \"[END]\"\n",
    "pad_token = \"[PAD]\"\n",
    "word_set.add(unknown_token)\n",
    "word_set.add(start_token)\n",
    "word_set.add(end_token)\n",
    "word_set.add(pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {}\n",
    "idx_to_word = [None] * len(word_set)\n",
    "index = 0\n",
    "for word in word_set:\n",
    "    word_to_idx[word] = index\n",
    "    idx_to_word[index] = word\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_idx = word_to_idx[unknown_token]\n",
    "start_idx = word_to_idx[start_token]\n",
    "end_idx = word_to_idx[end_token]\n",
    "pad_idx = word_to_idx[pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentences in data:\n",
    "    sentences[1].append(end_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['you', 'know', 'french'],\n",
       " ['sure', 'do', 'my', 'moms', 'from', 'canada', '[END]']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengths = [len(sentences[0]) for sentences in data]\n",
    "ground_truth_lengths = [len(sentences[1]) for sentences in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add padding\n",
    "max_input_lengths = max(input_lengths)\n",
    "max_ground_truth_lengths = max(ground_truth_lengths)\n",
    "input_sentences = []\n",
    "ground_truth_sentences = []\n",
    "for sentences in data:\n",
    "    input_sentences.append(sentences[0] + [pad_token]*(max_input_lengths-len(sentences[0])))\n",
    "    ground_truth_sentences.append(sentences[1] + [pad_token]*(max_ground_truth_lengths-len(sentences[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences_idx = [[word_to_idx[word] if word in word_to_idx else unknown_idx for word in sentence] for sentence in input_sentences]\n",
    "ground_truth_sentences_idx = [[word_to_idx[word] if word in word_to_idx else unknown_idx for word in sentence] for sentence in ground_truth_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences_idx = np.array(input_sentences_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_sentences_idx = np.array(ground_truth_sentences_idx)\n",
    "input_lengths = np.array(input_lengths)\n",
    "ground_truth_lengths = np.array(ground_truth_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['you', 'know', 'chastity'],\n",
       " ['i', 'believe', 'we', 'share', 'an', 'art', 'instructor', '[END]']]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(batch_size,\n",
    "                    input_sentences_idx,\n",
    "                    ground_truth_sentences_idx,\n",
    "                    input_lengths,\n",
    "                    ground_truth_lengths):\n",
    "    index = 0\n",
    "    while index < len(input_sentences_idx):\n",
    "        batch_input_length = input_lengths[index:index+batch_size]\n",
    "        batch_input_sentences_idx = input_sentences_idx[index:index+batch_size, :batch_input_length.max()]\n",
    "        batch_ground_truth_length = ground_truth_lengths[index:index+batch_size]\n",
    "        batch_ground_truth_sentences_idx = ground_truth_sentences_idx[index:index+batch_size, :batch_ground_truth_length.max()]\n",
    "        yield (batch_input_sentences_idx, batch_input_length,\n",
    "              batch_ground_truth_sentences_idx, batch_ground_truth_length)\n",
    "        index += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = batch_generator(10, input_sentences_idx, ground_truth_sentences_idx, input_lengths, ground_truth_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_set)\n",
    "num_units = 128\n",
    "embedding_size = 100\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prepare(text):\n",
    "    \"\"\"Performs tokenization and simple preprocessing.\"\"\"\n",
    "    \n",
    "    replace_by_space_re = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "    good_symbols_re = re.compile('[^0-9a-z #+_]')\n",
    "#     stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "    text = text.lower()\n",
    "    text = replace_by_space_re.sub(' ', text)\n",
    "    text = good_symbols_re.sub('', text)\n",
    "#     text = ' '.join([x for x in text.split() if x and x not in stopwords_set])\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    def __init__(self):\n",
    "        self.declare_placeholders()\n",
    "        self.build_input_encoder()\n",
    "        self.build_ground_truth_encoder()\n",
    "        self.build_hidden_state()\n",
    "        self.build_decoder()\n",
    "        self.define_loss_and_train()\n",
    "    \n",
    "    def declare_placeholders(self):\n",
    "        \"\"\"Specifies placeholders for the model.\"\"\"\n",
    "        # Placeholders for input and its actual lengths.\n",
    "        self.input_batch = tf.placeholder(shape=(None, None), dtype=tf.int32, name='input_batch')\n",
    "        self.input_batch_lengths = tf.placeholder(shape=(None, ), dtype=tf.int32, name='input_batch_lengths')\n",
    "\n",
    "        # Placeholders for groundtruth and its actual lengths.\n",
    "        self.ground_truth = tf.placeholder(shape=(None, None), dtype=tf.int32, name='ground_truth')\n",
    "        self.ground_truth_lengths = tf.placeholder(shape=(None, ), dtype=tf.int32, name='ground_truth_lengths')\n",
    "\n",
    "        self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
    "        self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "\n",
    "    def build_input_encoder(self):\n",
    "        with tf.variable_scope('input_encoder') as input_encoder_scope:\n",
    "            random_initializer = tf.random_uniform((vocab_size, embedding_size), -1.0, 1.0)\n",
    "            self.embeddings = tf.Variable(initial_value=random_initializer, name='embeddings', dtype=tf.float32) \n",
    "\n",
    "            # Perform embeddings lookup for self.input_batch. \n",
    "            self.input_batch_embedded = tf.nn.embedding_lookup(self.embeddings, self.input_batch)\n",
    "            # Create encoder cells\n",
    "            rnn_layers = []\n",
    "            for i in range(num_encoder_layers-1):\n",
    "                with tf.variable_scope('input_encoder_rnn_layer' + str(i + 1)) as scope:\n",
    "                    cell = tf.nn.rnn_cell.GRUCell(num_units, activation=tf.nn.relu)\n",
    "                    cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=self.dropout_ph, dtype=tf.float32)\n",
    "                    rnn_layers.append(cell)\n",
    "            with tf.variable_scope('input_encoder_rnn_layer' + str(num_encoder_layers)) as scope:\n",
    "                cell = tf.nn.rnn_cell.GRUCell(num_units)\n",
    "                cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=self.dropout_ph, dtype=tf.float32)\n",
    "                rnn_layers.append(cell)\n",
    "            encoder_cell = tf.contrib.rnn.MultiRNNCell(rnn_layers)\n",
    "            self.input_encoder_outputs, self.final_input_encoder_state = tf.nn.dynamic_rnn(\n",
    "                encoder_cell,\n",
    "                self.input_batch_embedded,\n",
    "                sequence_length=self.input_batch_lengths,\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            self.final_input_encoder_state = self.final_input_encoder_state[-1]\n",
    "\n",
    "    def build_ground_truth_encoder(self):\n",
    "        with tf.variable_scope('ground_truth_encoder') as ground_truth_encoder:\n",
    "            self.ground_truth_batch_embedded = tf.nn.embedding_lookup(self.embeddings, self.ground_truth)\n",
    "            rnn_layers = []\n",
    "            for i in range(num_encoder_layers-1):\n",
    "                with tf.variable_scope('ground_truth_encoder_rnn_layer' + str(i + 1)) as scope:\n",
    "                    cell = tf.nn.rnn_cell.GRUCell(num_units, activation=tf.nn.relu)\n",
    "                    cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=self.dropout_ph, dtype=tf.float32)\n",
    "                    rnn_layers.append(cell)\n",
    "            with tf.variable_scope('ground_truth_encoder_rnn_layer' + str(num_encoder_layers)) as scope:\n",
    "                cell = tf.nn.rnn_cell.GRUCell(num_units)\n",
    "                cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=self.dropout_ph, dtype=tf.float32)\n",
    "                rnn_layers.append(cell)\n",
    "            encoder_cell = tf.contrib.rnn.MultiRNNCell(rnn_layers)\n",
    "            _, self.final_ground_truth_encoder_state = tf.nn.dynamic_rnn(\n",
    "                encoder_cell,\n",
    "                self.ground_truth_batch_embedded,\n",
    "                sequence_length=self.ground_truth_lengths,\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            self.final_ground_truth_encoder_state = self.final_ground_truth_encoder_state[-1]\n",
    "\n",
    "    def build_hidden_state(self):\n",
    "        self.z_mean_from_input, self.z_log_var_from_input = tf.split(\n",
    "            self.final_input_encoder_state, num_or_size_splits=2, axis=1)\n",
    "        self.z_mean_from_ground_truth, self.z_log_var_from_grount_truth = tf.split(\n",
    "            self.final_input_encoder_state, num_or_size_splits=2, axis=1)\n",
    "        self.z = (self.z_mean_from_ground_truth +\n",
    "                  tf.exp(0.5*self.z_log_var_from_grount_truth) *\n",
    "                  tf.random_normal(tf.shape(self.z_log_var_from_grount_truth), 0, 1, dtype=tf.float32))\n",
    "\n",
    "    def build_decoder(self):\n",
    "        batch_size = tf.shape(self.input_batch)[0]\n",
    "        start_tokens = tf.fill([batch_size], start_idx)\n",
    "        ground_truth_as_input = tf.concat([tf.expand_dims(start_tokens, 1), self.ground_truth], 1)\n",
    "        self.ground_truth_embedded = tf.nn.embedding_lookup(\n",
    "            self.embeddings, ground_truth_as_input)\n",
    "        train_helper = tf.contrib.seq2seq.TrainingHelper(self.ground_truth_embedded,\n",
    "                                                         self.ground_truth_lengths)\n",
    "        infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(self.embeddings, start_tokens, end_idx)\n",
    "\n",
    "        def decode(helper, scope, reuse=None):\n",
    "            \"\"\"Creates decoder and return the results of the decoding with a given helper.\"\"\"\n",
    "            with tf.variable_scope(scope, reuse=reuse):\n",
    "                # Create GRUCell with dropout. Do not forget to set the reuse flag properly.\n",
    "                rnn_layers = []\n",
    "                for i in range(num_decoder_layers-1):\n",
    "                    with tf.variable_scope('decoder_rnn_layer' + str(i + 1)) as scope:\n",
    "                        decoder_cell = tf.contrib.rnn.GRUCell(num_units=num_units/2, reuse=reuse, activation=tf.nn.tanh)\n",
    "                        decoder_cell = tf.contrib.rnn.DropoutWrapper(decoder_cell, input_keep_prob=self.dropout_ph)\n",
    "                        rnn_layers.append(decoder_cell)\n",
    "                with tf.variable_scope('decoder_rnn_layer' + str(num_decoder_layers)) as scope:\n",
    "                    decoder_cell = tf.contrib.rnn.GRUCell(num_units=num_units/2, reuse=reuse)\n",
    "                    decoder_cell = tf.contrib.rnn.DropoutWrapper(decoder_cell, input_keep_prob=self.dropout_ph)\n",
    "                    rnn_layers.append(decoder_cell)\n",
    "                decoder_cell = tf.contrib.rnn.MultiRNNCell(rnn_layers)\n",
    "                # Create attention\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    num_units=num_units, memory=tf.split(self.input_encoder_outputs, num_or_size_splits=2, axis=-1)[0],\n",
    "                    memory_sequence_length=self.input_batch_lengths)\n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                    decoder_cell, attention_mechanism, attention_layer_size=num_units)\n",
    "                # Create a projection wrapper.\n",
    "                decoder_cell = tf.contrib.rnn.OutputProjectionWrapper(decoder_cell, vocab_size, reuse=reuse)\n",
    "                # Create BasicDecoder, pass the defined cell, a helper, and initial state.\n",
    "                # The initial state should be equal to the final state of the encoder!\n",
    "                second_state = tf.zeros((1, num_units/2))\n",
    "                second_state = tf.tile(second_state, [batch_size, 1])\n",
    "                decoder_initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=batch_size).clone(\n",
    "                    cell_state=(self.z, second_state))\n",
    "                decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_cell, helper=helper, initial_state=decoder_initial_state)\n",
    "\n",
    "                # The first returning argument of dynamic_decode contains two fields:\n",
    "                #   rnn_output (predicted logits)\n",
    "                #   sample_id (predictions)\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=decoder, maximum_iterations=tf.reduce_max(self.ground_truth_lengths), \n",
    "                                                                  output_time_major=False, impute_finished=True)\n",
    "\n",
    "                return outputs\n",
    "\n",
    "        self.train_outputs = decode(train_helper, 'decode')\n",
    "        self.infer_outputs = decode(infer_helper, 'decode', reuse=True)\n",
    "        self.train_predictions = self.train_outputs.sample_id\n",
    "        self.infer_predictions = self.infer_outputs.sample_id\n",
    "\n",
    "    def define_loss_and_train(self):\n",
    "        weights = tf.cast(tf.sequence_mask(self.ground_truth_lengths), dtype=tf.float32)\n",
    "        self.reconstruction_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            self.train_outputs.rnn_output,\n",
    "            self.ground_truth,\n",
    "            weights\n",
    "        )\n",
    "        self.kl_loss = (0.5*(self.z_log_var_from_input-self.z_log_var_from_grount_truth)+\n",
    "                       (tf.exp(self.z_log_var_from_grount_truth)+(self.z_mean_from_ground_truth-self.z_mean_from_input)**2)/\n",
    "                       (2*tf.exp(self.z_log_var_from_input))-0.5)\n",
    "        self.kl_loss = tf.reduce_mean(self.kl_loss)\n",
    "        self.loss = self.kl_loss + self.reconstruction_loss\n",
    "        self.train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=self.loss,\n",
    "            optimizer='Adam',\n",
    "            learning_rate=self.learning_rate_ph,\n",
    "            clip_gradients=1.0,\n",
    "            global_step=tf.train.get_global_step()\n",
    "        )\n",
    "\n",
    "    def train_on_batch(self, session, X, X_seq_len, Y, Y_seq_len, learning_rate, dropout_keep_probability):\n",
    "        feed_dict = {\n",
    "            self.input_batch: X,\n",
    "            self.input_batch_lengths: X_seq_len,\n",
    "            self.ground_truth: Y,\n",
    "            self.ground_truth_lengths: Y_seq_len,\n",
    "            self.learning_rate_ph: learning_rate,\n",
    "            self.dropout_ph: dropout_keep_probability\n",
    "        }\n",
    "        loss, _ = session.run([\n",
    "            self.loss,\n",
    "            self.train_op], feed_dict=feed_dict)\n",
    "        return loss\n",
    "    \n",
    "    def get_reply(self, session, input_sentence):\n",
    "        input_sentence = text_prepare(input_sentence)\n",
    "        X = [[word_to_idx[word] if word in word_to_idx else unknown_idx for word in input_sentence]]\n",
    "        X = np.array(X)\n",
    "        feed_dict = {\n",
    "            self.input_batch: X,\n",
    "            self.input_batch_lengths: np.array([len(input_sentence)])\n",
    "        }\n",
    "        z_mean, z_log_var, input_encoder_outputs = session.run([\n",
    "            self.z_mean_from_input,\n",
    "            self.z_log_var_from_input,\n",
    "            self.input_encoder_outputs\n",
    "        ], feed_dict=feed_dict)\n",
    "        z_mean = z_mean[0]\n",
    "        z_log_var = z_log_var[0]\n",
    "        z = np.random.normal(z_mean, np.exp(0.5*z_log_var), z_mean.size)\n",
    "        z = z[np.newaxis,:]\n",
    "        feed_dict = {\n",
    "            self.z: z,\n",
    "            self.input_batch: X,\n",
    "            self.input_batch_lengths: np.array([len(input_sentence)]),\n",
    "            self.ground_truth_lengths: np.array([15])\n",
    "        }\n",
    "        pred = session.run([self.infer_predictions], feed_dict=feed_dict)\n",
    "        return \" \".join([idx_to_word[index] for index in pred[0][0]])\n",
    "\n",
    "    def train(self, session, epochs, batch_size, input_sentences_idx, ground_truth_sentences_idx, input_lengths, ground_truth_lengths, learning_rate, dropout_keep_probability):\n",
    "        for i in range(epochs):\n",
    "            batch_num = 1\n",
    "            for (batch_input_sentences_idx,\n",
    "                 batch_input_length,\n",
    "                 batch_ground_truth_sentences_idx,\n",
    "                 batch_ground_truth_length) in batch_generator(\n",
    "                batch_size, input_sentences_idx, ground_truth_sentences_idx,\n",
    "                input_lengths, ground_truth_lengths):\n",
    "                loss = self.train_on_batch(\n",
    "                    session,\n",
    "                    batch_input_sentences_idx,\n",
    "                    batch_input_length,\n",
    "                    batch_ground_truth_sentences_idx,\n",
    "                    batch_ground_truth_length,\n",
    "                    learning_rate,\n",
    "                    dropout_keep_probability\n",
    "                )\n",
    "                print(\"Epoch {i}, batch {batch}, loss = {loss}\".format(i=i+1, batch=batch_num, loss=loss))\n",
    "                batch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = ChatBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 1, loss = 9.453303337097168\n",
      "Epoch 1, batch 2, loss = 9.452513694763184\n",
      "Epoch 1, batch 3, loss = 9.452415466308594\n",
      "Epoch 1, batch 4, loss = 9.451004981994629\n",
      "Epoch 1, batch 5, loss = 9.45026683807373\n",
      "Epoch 1, batch 6, loss = 9.451022148132324\n",
      "Epoch 1, batch 7, loss = 9.449259757995605\n",
      "Epoch 1, batch 8, loss = 9.449374198913574\n",
      "Epoch 1, batch 9, loss = 9.447999000549316\n",
      "Epoch 1, batch 10, loss = 9.447127342224121\n",
      "Epoch 1, batch 11, loss = 9.445918083190918\n",
      "Epoch 1, batch 12, loss = 9.445520401000977\n",
      "Epoch 1, batch 13, loss = 9.44456672668457\n",
      "Epoch 1, batch 14, loss = 9.443458557128906\n",
      "Epoch 1, batch 15, loss = 9.443755149841309\n",
      "Epoch 1, batch 16, loss = 9.440383911132812\n",
      "Epoch 1, batch 17, loss = 9.441139221191406\n",
      "Epoch 1, batch 18, loss = 9.440202713012695\n",
      "Epoch 1, batch 19, loss = 9.43825912475586\n",
      "Epoch 1, batch 20, loss = 9.438660621643066\n",
      "Epoch 1, batch 21, loss = 9.434859275817871\n",
      "Epoch 1, batch 22, loss = 9.434871673583984\n",
      "Epoch 1, batch 23, loss = 9.433428764343262\n",
      "Epoch 1, batch 24, loss = 9.43419361114502\n",
      "Epoch 1, batch 25, loss = 9.430374145507812\n",
      "Epoch 1, batch 26, loss = 9.428886413574219\n",
      "Epoch 1, batch 27, loss = 9.426321983337402\n",
      "Epoch 1, batch 28, loss = 9.42543888092041\n",
      "Epoch 1, batch 29, loss = 9.424110412597656\n",
      "Epoch 1, batch 30, loss = 9.420875549316406\n",
      "Epoch 1, batch 31, loss = 9.419827461242676\n",
      "Epoch 1, batch 32, loss = 9.41698932647705\n",
      "Epoch 1, batch 33, loss = 9.413461685180664\n",
      "Epoch 1, batch 34, loss = 9.41169548034668\n",
      "Epoch 1, batch 35, loss = 9.409411430358887\n",
      "Epoch 1, batch 36, loss = 9.404243469238281\n",
      "Epoch 1, batch 37, loss = 9.399168014526367\n",
      "Epoch 1, batch 38, loss = 9.39298152923584\n",
      "Epoch 1, batch 39, loss = 9.393091201782227\n",
      "Epoch 1, batch 40, loss = 9.388533592224121\n",
      "Epoch 1, batch 41, loss = 9.384044647216797\n",
      "Epoch 1, batch 42, loss = 9.379496574401855\n",
      "Epoch 1, batch 43, loss = 9.37448787689209\n",
      "Epoch 1, batch 44, loss = 9.369986534118652\n",
      "Epoch 1, batch 45, loss = 9.361842155456543\n",
      "Epoch 1, batch 46, loss = 9.35617733001709\n",
      "Epoch 1, batch 47, loss = 9.35886287689209\n",
      "Epoch 1, batch 48, loss = 9.342065811157227\n",
      "Epoch 1, batch 49, loss = 9.342963218688965\n",
      "Epoch 1, batch 50, loss = 9.325066566467285\n",
      "Epoch 1, batch 51, loss = 9.318351745605469\n",
      "Epoch 1, batch 52, loss = 9.287030220031738\n",
      "Epoch 1, batch 53, loss = 9.264579772949219\n",
      "Epoch 1, batch 54, loss = 9.255870819091797\n",
      "Epoch 1, batch 55, loss = 9.23436164855957\n",
      "Epoch 1, batch 56, loss = 9.211920738220215\n",
      "Epoch 1, batch 57, loss = 9.204955101013184\n",
      "Epoch 1, batch 58, loss = 9.18338680267334\n",
      "Epoch 1, batch 59, loss = 9.16238784790039\n",
      "Epoch 1, batch 60, loss = 9.14112663269043\n",
      "Epoch 1, batch 61, loss = 9.115676879882812\n",
      "Epoch 1, batch 62, loss = 9.097519874572754\n",
      "Epoch 1, batch 63, loss = 9.074368476867676\n",
      "Epoch 1, batch 64, loss = 9.042315483093262\n",
      "Epoch 1, batch 65, loss = 8.993786811828613\n",
      "Epoch 1, batch 66, loss = 9.000630378723145\n",
      "Epoch 1, batch 67, loss = 8.931134223937988\n",
      "Epoch 1, batch 68, loss = 8.900801658630371\n",
      "Epoch 1, batch 69, loss = 8.884058952331543\n",
      "Epoch 1, batch 70, loss = 8.861968994140625\n",
      "Epoch 1, batch 71, loss = 8.841402053833008\n",
      "Epoch 1, batch 72, loss = 8.77170181274414\n",
      "Epoch 1, batch 73, loss = 8.742130279541016\n",
      "Epoch 1, batch 74, loss = 8.69234561920166\n",
      "Epoch 1, batch 75, loss = 8.635095596313477\n",
      "Epoch 1, batch 76, loss = 8.571724891662598\n",
      "Epoch 1, batch 77, loss = 8.553072929382324\n",
      "Epoch 1, batch 78, loss = 8.49733829498291\n",
      "Epoch 1, batch 79, loss = 8.428170204162598\n",
      "Epoch 1, batch 80, loss = 8.36864948272705\n",
      "Epoch 1, batch 81, loss = 8.34664249420166\n",
      "Epoch 1, batch 82, loss = 8.323026657104492\n",
      "Epoch 1, batch 83, loss = 8.221860885620117\n",
      "Epoch 1, batch 84, loss = 8.15678882598877\n",
      "Epoch 1, batch 85, loss = 8.11860466003418\n",
      "Epoch 1, batch 86, loss = 8.056077003479004\n",
      "Epoch 1, batch 87, loss = 7.9894208908081055\n",
      "Epoch 1, batch 88, loss = 7.882772922515869\n",
      "Epoch 1, batch 89, loss = 7.857985019683838\n",
      "Epoch 1, batch 90, loss = 7.832383632659912\n",
      "Epoch 1, batch 91, loss = 7.6922383308410645\n",
      "Epoch 1, batch 92, loss = 7.699244022369385\n",
      "Epoch 1, batch 93, loss = 7.580963611602783\n",
      "Epoch 1, batch 94, loss = 7.522149562835693\n",
      "Epoch 1, batch 95, loss = 7.502668380737305\n",
      "Epoch 1, batch 96, loss = 7.421290874481201\n",
      "Epoch 1, batch 97, loss = 7.368239879608154\n",
      "Epoch 1, batch 98, loss = 7.326298236846924\n",
      "Epoch 1, batch 99, loss = 7.197381496429443\n",
      "Epoch 1, batch 100, loss = 7.186375617980957\n",
      "Epoch 1, batch 101, loss = 7.134357929229736\n",
      "Epoch 1, batch 102, loss = 7.04680061340332\n",
      "Epoch 1, batch 103, loss = 6.998374938964844\n",
      "Epoch 1, batch 104, loss = 6.919794082641602\n",
      "Epoch 1, batch 105, loss = 6.843299865722656\n",
      "Epoch 1, batch 106, loss = 6.788420677185059\n",
      "Epoch 1, batch 107, loss = 6.706778526306152\n",
      "Epoch 1, batch 108, loss = 6.738030433654785\n",
      "Epoch 1, batch 109, loss = 6.622894763946533\n",
      "Epoch 1, batch 110, loss = 6.452929496765137\n",
      "Epoch 1, batch 111, loss = 6.478458404541016\n",
      "Epoch 1, batch 112, loss = 6.3287811279296875\n",
      "Epoch 1, batch 113, loss = 6.322628021240234\n",
      "Epoch 1, batch 114, loss = 6.313314437866211\n",
      "Epoch 1, batch 115, loss = 6.255166053771973\n",
      "Epoch 1, batch 116, loss = 6.229925632476807\n",
      "Epoch 1, batch 117, loss = 6.212789058685303\n",
      "Epoch 1, batch 118, loss = 6.198770046234131\n",
      "Epoch 1, batch 119, loss = 6.193308353424072\n",
      "Epoch 1, batch 120, loss = 6.15203332901001\n",
      "Epoch 1, batch 121, loss = 6.153126239776611\n",
      "Epoch 1, batch 122, loss = 6.185155868530273\n",
      "Epoch 1, batch 123, loss = 6.023561000823975\n",
      "Epoch 1, batch 124, loss = 6.1591715812683105\n",
      "Epoch 1, batch 125, loss = 6.2080841064453125\n",
      "Epoch 1, batch 126, loss = 6.354538440704346\n",
      "Epoch 1, batch 127, loss = 6.025934219360352\n",
      "Epoch 1, batch 128, loss = 6.18392276763916\n",
      "Epoch 1, batch 129, loss = 6.0978851318359375\n",
      "Epoch 1, batch 130, loss = 6.009945392608643\n",
      "Epoch 1, batch 131, loss = 6.096776485443115\n",
      "Epoch 1, batch 132, loss = 6.097862720489502\n",
      "Epoch 1, batch 133, loss = 5.938511371612549\n",
      "Epoch 1, batch 134, loss = 6.032160758972168\n",
      "Epoch 1, batch 135, loss = 6.180395126342773\n",
      "Epoch 1, batch 136, loss = 6.074896335601807\n",
      "Epoch 1, batch 137, loss = 6.080053329467773\n",
      "Epoch 1, batch 138, loss = 6.222009181976318\n",
      "Epoch 1, batch 139, loss = 6.177328586578369\n",
      "Epoch 1, batch 140, loss = 5.982580661773682\n",
      "Epoch 1, batch 141, loss = 5.895750522613525\n",
      "Epoch 1, batch 142, loss = 6.111423492431641\n",
      "Epoch 1, batch 143, loss = 6.033626556396484\n",
      "Epoch 1, batch 144, loss = 6.0300211906433105\n",
      "Epoch 1, batch 145, loss = 6.0338826179504395\n",
      "Epoch 1, batch 146, loss = 5.927138805389404\n",
      "Epoch 1, batch 147, loss = 6.019266605377197\n",
      "Epoch 1, batch 148, loss = 6.13310432434082\n",
      "Epoch 1, batch 149, loss = 6.065512657165527\n",
      "Epoch 1, batch 150, loss = 6.203711032867432\n",
      "Epoch 1, batch 151, loss = 6.011177062988281\n",
      "Epoch 1, batch 152, loss = 5.930235862731934\n",
      "Epoch 1, batch 153, loss = 6.149346351623535\n",
      "Epoch 1, batch 154, loss = 6.047908782958984\n",
      "Epoch 1, batch 155, loss = 6.177234172821045\n",
      "Epoch 1, batch 156, loss = 6.064328193664551\n",
      "Epoch 1, batch 157, loss = 6.039775848388672\n",
      "Epoch 1, batch 158, loss = 6.042511463165283\n",
      "Epoch 1, batch 159, loss = 5.9774699211120605\n",
      "Epoch 1, batch 160, loss = 6.0185394287109375\n",
      "Epoch 1, batch 161, loss = 6.103022575378418\n",
      "Epoch 1, batch 162, loss = 6.134611129760742\n",
      "Epoch 1, batch 163, loss = 5.952300548553467\n",
      "Epoch 1, batch 164, loss = 6.026043891906738\n",
      "Epoch 1, batch 165, loss = 6.094919204711914\n",
      "Epoch 1, batch 166, loss = 5.947904586791992\n",
      "Epoch 1, batch 167, loss = 5.959652900695801\n",
      "Epoch 1, batch 168, loss = 6.012446403503418\n",
      "Epoch 1, batch 169, loss = 6.142329216003418\n",
      "Epoch 1, batch 170, loss = 6.001862525939941\n",
      "Epoch 1, batch 171, loss = 6.074166774749756\n",
      "Epoch 1, batch 172, loss = 6.160284996032715\n",
      "Epoch 1, batch 173, loss = 5.985513210296631\n",
      "Epoch 1, batch 174, loss = 5.963837146759033\n",
      "Epoch 1, batch 175, loss = 6.312697410583496\n",
      "Epoch 1, batch 176, loss = 6.040116310119629\n",
      "Epoch 1, batch 177, loss = 5.93873929977417\n",
      "Epoch 1, batch 178, loss = 6.026000499725342\n",
      "Epoch 1, batch 179, loss = 6.102116107940674\n",
      "Epoch 1, batch 180, loss = 6.102827548980713\n",
      "Epoch 1, batch 181, loss = 6.00410795211792\n",
      "Epoch 1, batch 182, loss = 5.950168609619141\n",
      "Epoch 1, batch 183, loss = 5.98928165435791\n",
      "Epoch 1, batch 184, loss = 6.132051944732666\n",
      "Epoch 1, batch 185, loss = 5.970556735992432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 186, loss = 6.146169185638428\n",
      "Epoch 1, batch 187, loss = 6.076430797576904\n",
      "Epoch 1, batch 188, loss = 5.835185527801514\n",
      "Epoch 1, batch 189, loss = 5.927221775054932\n",
      "Epoch 1, batch 190, loss = 6.0712175369262695\n",
      "Epoch 1, batch 191, loss = 5.911068439483643\n",
      "Epoch 1, batch 192, loss = 6.120402812957764\n",
      "Epoch 1, batch 193, loss = 5.881659030914307\n",
      "Epoch 1, batch 194, loss = 6.011129856109619\n",
      "Epoch 1, batch 195, loss = 5.939219951629639\n",
      "Epoch 1, batch 196, loss = 6.008031368255615\n",
      "Epoch 1, batch 197, loss = 6.139239311218262\n",
      "Epoch 1, batch 198, loss = 6.07028341293335\n",
      "Epoch 1, batch 199, loss = 5.934192180633545\n",
      "Epoch 1, batch 200, loss = 6.002211570739746\n",
      "Epoch 1, batch 201, loss = 5.991769313812256\n",
      "Epoch 1, batch 202, loss = 5.949981212615967\n",
      "Epoch 1, batch 203, loss = 5.931615352630615\n",
      "Epoch 1, batch 204, loss = 6.038876056671143\n",
      "Epoch 1, batch 205, loss = 5.948448657989502\n",
      "Epoch 1, batch 206, loss = 6.0603928565979\n",
      "Epoch 1, batch 207, loss = 6.2041521072387695\n",
      "Epoch 1, batch 208, loss = 6.014712810516357\n",
      "Epoch 1, batch 209, loss = 6.0562334060668945\n",
      "Epoch 1, batch 210, loss = 6.080216407775879\n",
      "Epoch 1, batch 211, loss = 6.0390238761901855\n",
      "Epoch 1, batch 212, loss = 6.038918972015381\n",
      "Epoch 1, batch 213, loss = 5.964491367340088\n",
      "Epoch 1, batch 214, loss = 6.038026332855225\n",
      "Epoch 1, batch 215, loss = 6.033040523529053\n",
      "Epoch 1, batch 216, loss = 5.968857765197754\n",
      "Epoch 1, batch 217, loss = 5.979133129119873\n",
      "Epoch 1, batch 218, loss = 6.0361785888671875\n",
      "Epoch 1, batch 219, loss = 6.226561069488525\n",
      "Epoch 1, batch 220, loss = 5.953303337097168\n",
      "Epoch 1, batch 221, loss = 5.978616714477539\n",
      "Epoch 1, batch 222, loss = 6.128720760345459\n",
      "Epoch 1, batch 223, loss = 5.9798054695129395\n",
      "Epoch 1, batch 224, loss = 5.9676289558410645\n",
      "Epoch 1, batch 225, loss = 5.85813045501709\n",
      "Epoch 1, batch 226, loss = 6.020562648773193\n",
      "Epoch 1, batch 227, loss = 6.092581272125244\n",
      "Epoch 1, batch 228, loss = 6.045456886291504\n",
      "Epoch 1, batch 229, loss = 6.073878288269043\n",
      "Epoch 1, batch 230, loss = 6.0020270347595215\n",
      "Epoch 1, batch 231, loss = 5.9267778396606445\n",
      "Epoch 1, batch 232, loss = 5.955350399017334\n",
      "Epoch 1, batch 233, loss = 5.980154037475586\n",
      "Epoch 1, batch 234, loss = 5.983504295349121\n",
      "Epoch 1, batch 235, loss = 5.94633674621582\n",
      "Epoch 1, batch 236, loss = 5.989333629608154\n",
      "Epoch 1, batch 237, loss = 6.047357082366943\n",
      "Epoch 1, batch 238, loss = 5.939328670501709\n",
      "Epoch 1, batch 239, loss = 6.141907215118408\n",
      "Epoch 1, batch 240, loss = 6.111929893493652\n",
      "Epoch 1, batch 241, loss = 6.021113395690918\n",
      "Epoch 1, batch 242, loss = 5.9521050453186035\n",
      "Epoch 1, batch 243, loss = 6.088305950164795\n",
      "Epoch 1, batch 244, loss = 5.915287971496582\n",
      "Epoch 1, batch 245, loss = 5.919970512390137\n",
      "Epoch 1, batch 246, loss = 6.0658721923828125\n",
      "Epoch 1, batch 247, loss = 5.972622871398926\n",
      "Epoch 1, batch 248, loss = 6.067428112030029\n",
      "Epoch 1, batch 249, loss = 5.9369916915893555\n",
      "Epoch 1, batch 250, loss = 5.923692226409912\n",
      "Epoch 1, batch 251, loss = 5.919852256774902\n",
      "Epoch 1, batch 252, loss = 6.011422157287598\n",
      "Epoch 1, batch 253, loss = 5.973409652709961\n",
      "Epoch 1, batch 254, loss = 5.962481498718262\n",
      "Epoch 1, batch 255, loss = 6.072977542877197\n",
      "Epoch 1, batch 256, loss = 5.989707946777344\n",
      "Epoch 1, batch 257, loss = 5.976700782775879\n",
      "Epoch 1, batch 258, loss = 6.0563507080078125\n",
      "Epoch 1, batch 259, loss = 5.968141555786133\n",
      "Epoch 1, batch 260, loss = 5.942949295043945\n",
      "Epoch 1, batch 261, loss = 6.046805381774902\n",
      "Epoch 1, batch 262, loss = 6.019826889038086\n",
      "Epoch 1, batch 263, loss = 6.053712844848633\n",
      "Epoch 1, batch 264, loss = 5.951563835144043\n",
      "Epoch 1, batch 265, loss = 5.851466178894043\n",
      "Epoch 1, batch 266, loss = 6.032830715179443\n",
      "Epoch 1, batch 267, loss = 6.026564121246338\n",
      "Epoch 1, batch 268, loss = 5.950512409210205\n",
      "Epoch 1, batch 269, loss = 6.041351318359375\n",
      "Epoch 1, batch 270, loss = 6.152496814727783\n",
      "Epoch 1, batch 271, loss = 6.031968116760254\n",
      "Epoch 1, batch 272, loss = 6.055016994476318\n",
      "Epoch 1, batch 273, loss = 5.959194660186768\n",
      "Epoch 1, batch 274, loss = 5.970905780792236\n",
      "Epoch 1, batch 275, loss = 6.061263084411621\n",
      "Epoch 1, batch 276, loss = 5.9955573081970215\n",
      "Epoch 1, batch 277, loss = 6.130367279052734\n",
      "Epoch 1, batch 278, loss = 5.977630615234375\n",
      "Epoch 1, batch 279, loss = 5.987964630126953\n",
      "Epoch 1, batch 280, loss = 6.080167293548584\n",
      "Epoch 1, batch 281, loss = 6.003422737121582\n",
      "Epoch 1, batch 282, loss = 6.031703948974609\n",
      "Epoch 1, batch 283, loss = 5.8613691329956055\n",
      "Epoch 1, batch 284, loss = 5.934775352478027\n",
      "Epoch 1, batch 285, loss = 5.942907810211182\n",
      "Epoch 1, batch 286, loss = 6.091212272644043\n",
      "Epoch 1, batch 287, loss = 5.914905071258545\n",
      "Epoch 1, batch 288, loss = 5.955469131469727\n",
      "Epoch 1, batch 289, loss = 6.033411979675293\n",
      "Epoch 1, batch 290, loss = 5.925055503845215\n",
      "Epoch 1, batch 291, loss = 5.979567527770996\n",
      "Epoch 1, batch 292, loss = 6.003694534301758\n",
      "Epoch 1, batch 293, loss = 5.963304042816162\n",
      "Epoch 1, batch 294, loss = 5.971940994262695\n",
      "Epoch 1, batch 295, loss = 5.964298248291016\n",
      "Epoch 1, batch 296, loss = 6.000633716583252\n",
      "Epoch 1, batch 297, loss = 6.032528877258301\n",
      "Epoch 1, batch 298, loss = 5.956974029541016\n",
      "Epoch 1, batch 299, loss = 6.076998710632324\n",
      "Epoch 1, batch 300, loss = 5.931014537811279\n",
      "Epoch 1, batch 301, loss = 6.078186511993408\n",
      "Epoch 1, batch 302, loss = 5.974574089050293\n",
      "Epoch 1, batch 303, loss = 5.977075099945068\n",
      "Epoch 1, batch 304, loss = 6.038755893707275\n",
      "Epoch 1, batch 305, loss = 5.9904022216796875\n",
      "Epoch 1, batch 306, loss = 6.008005142211914\n",
      "Epoch 1, batch 307, loss = 6.019062042236328\n",
      "Epoch 1, batch 308, loss = 5.986918926239014\n",
      "Epoch 1, batch 309, loss = 5.989481449127197\n",
      "Epoch 1, batch 310, loss = 5.9850993156433105\n",
      "Epoch 1, batch 311, loss = 5.991605758666992\n",
      "Epoch 1, batch 312, loss = 6.027729511260986\n",
      "Epoch 1, batch 313, loss = 6.088139057159424\n",
      "Epoch 1, batch 314, loss = 6.0428571701049805\n",
      "Epoch 1, batch 315, loss = 5.910087585449219\n",
      "Epoch 1, batch 316, loss = 5.976990222930908\n",
      "Epoch 1, batch 317, loss = 5.965319633483887\n",
      "Epoch 1, batch 318, loss = 6.042332649230957\n",
      "Epoch 1, batch 319, loss = 5.932194709777832\n",
      "Epoch 1, batch 320, loss = 6.087939739227295\n",
      "Epoch 1, batch 321, loss = 5.902477264404297\n",
      "Epoch 1, batch 322, loss = 5.907192230224609\n",
      "Epoch 1, batch 323, loss = 6.017436981201172\n",
      "Epoch 1, batch 324, loss = 5.979101657867432\n",
      "Epoch 1, batch 325, loss = 5.976456642150879\n",
      "Epoch 1, batch 326, loss = 5.930883407592773\n",
      "Epoch 1, batch 327, loss = 6.0071001052856445\n",
      "Epoch 1, batch 328, loss = 6.001603126525879\n",
      "Epoch 1, batch 329, loss = 6.012500286102295\n",
      "Epoch 1, batch 330, loss = 6.050683498382568\n",
      "Epoch 1, batch 331, loss = 6.041009902954102\n",
      "Epoch 1, batch 332, loss = 6.041733741760254\n",
      "Epoch 1, batch 333, loss = 5.989727973937988\n",
      "Epoch 1, batch 334, loss = 6.1080427169799805\n",
      "Epoch 1, batch 335, loss = 5.991104602813721\n",
      "Epoch 1, batch 336, loss = 5.989635467529297\n",
      "Epoch 1, batch 337, loss = 6.028823375701904\n",
      "Epoch 1, batch 338, loss = 5.966473579406738\n",
      "Epoch 1, batch 339, loss = 6.059696674346924\n",
      "Epoch 1, batch 340, loss = 5.940069675445557\n",
      "Epoch 1, batch 341, loss = 6.030657768249512\n",
      "Epoch 1, batch 342, loss = 5.923901557922363\n",
      "Epoch 1, batch 343, loss = 6.0261945724487305\n",
      "Epoch 1, batch 344, loss = 5.966662883758545\n",
      "Epoch 1, batch 345, loss = 6.075794696807861\n",
      "Epoch 1, batch 346, loss = 5.995029449462891\n",
      "Epoch 1, batch 347, loss = 5.905889511108398\n",
      "Epoch 1, batch 348, loss = 5.980678081512451\n",
      "Epoch 1, batch 349, loss = 6.025854587554932\n",
      "Epoch 1, batch 350, loss = 5.947731971740723\n",
      "Epoch 1, batch 351, loss = 6.015852451324463\n",
      "Epoch 1, batch 352, loss = 6.01075553894043\n",
      "Epoch 1, batch 353, loss = 5.894426345825195\n",
      "Epoch 1, batch 354, loss = 6.093331336975098\n",
      "Epoch 1, batch 355, loss = 5.943340301513672\n",
      "Epoch 1, batch 356, loss = 6.080324172973633\n",
      "Epoch 1, batch 357, loss = 5.920785903930664\n",
      "Epoch 1, batch 358, loss = 5.999916076660156\n",
      "Epoch 1, batch 359, loss = 5.953862190246582\n",
      "Epoch 1, batch 360, loss = 6.029099464416504\n",
      "Epoch 1, batch 361, loss = 5.948400974273682\n",
      "Epoch 1, batch 362, loss = 6.0257368087768555\n",
      "Epoch 1, batch 363, loss = 5.998403072357178\n",
      "Epoch 1, batch 364, loss = 5.905304431915283\n",
      "Epoch 1, batch 365, loss = 5.846314430236816\n",
      "Epoch 1, batch 366, loss = 6.004948616027832\n",
      "Epoch 1, batch 367, loss = 5.908811569213867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 368, loss = 5.947820663452148\n",
      "Epoch 1, batch 369, loss = 6.026201248168945\n",
      "Epoch 1, batch 370, loss = 6.0012407302856445\n",
      "Epoch 1, batch 371, loss = 6.122353553771973\n",
      "Epoch 1, batch 372, loss = 5.976785659790039\n",
      "Epoch 1, batch 373, loss = 5.915890693664551\n",
      "Epoch 1, batch 374, loss = 5.862168788909912\n",
      "Epoch 1, batch 375, loss = 6.053094863891602\n",
      "Epoch 1, batch 376, loss = 5.930954933166504\n",
      "Epoch 1, batch 377, loss = 5.8758225440979\n",
      "Epoch 1, batch 378, loss = 5.982545375823975\n",
      "Epoch 1, batch 379, loss = 5.898486137390137\n",
      "Epoch 1, batch 380, loss = 6.026061058044434\n",
      "Epoch 1, batch 381, loss = 5.844474792480469\n",
      "Epoch 1, batch 382, loss = 5.971642017364502\n",
      "Epoch 1, batch 383, loss = 5.99247932434082\n",
      "Epoch 1, batch 384, loss = 5.927855014801025\n",
      "Epoch 1, batch 385, loss = 5.959246635437012\n",
      "Epoch 1, batch 386, loss = 6.012385845184326\n",
      "Epoch 1, batch 387, loss = 5.919038772583008\n",
      "Epoch 1, batch 388, loss = 5.9660234451293945\n",
      "Epoch 1, batch 389, loss = 5.877278804779053\n",
      "Epoch 1, batch 390, loss = 5.941601753234863\n",
      "Epoch 1, batch 391, loss = 5.953430652618408\n",
      "Epoch 1, batch 392, loss = 5.945584774017334\n",
      "Epoch 1, batch 393, loss = 6.022339344024658\n",
      "Epoch 1, batch 394, loss = 5.987196445465088\n",
      "Epoch 1, batch 395, loss = 5.883762836456299\n",
      "Epoch 1, batch 396, loss = 5.913161754608154\n",
      "Epoch 1, batch 397, loss = 5.952524185180664\n",
      "Epoch 1, batch 398, loss = 5.910350322723389\n",
      "Epoch 1, batch 399, loss = 6.033252239227295\n",
      "Epoch 1, batch 400, loss = 6.01343297958374\n",
      "Epoch 1, batch 401, loss = 6.004797458648682\n",
      "Epoch 1, batch 402, loss = 6.018858432769775\n",
      "Epoch 1, batch 403, loss = 5.98333740234375\n",
      "Epoch 1, batch 404, loss = 5.948269367218018\n",
      "Epoch 1, batch 405, loss = 5.899611949920654\n",
      "Epoch 1, batch 406, loss = 6.046480655670166\n",
      "Epoch 1, batch 407, loss = 5.985371112823486\n",
      "Epoch 1, batch 408, loss = 5.904830455780029\n",
      "Epoch 1, batch 409, loss = 6.017173767089844\n",
      "Epoch 1, batch 410, loss = 5.873171329498291\n",
      "Epoch 1, batch 411, loss = 6.051984786987305\n",
      "Epoch 1, batch 412, loss = 6.033324718475342\n",
      "Epoch 1, batch 413, loss = 5.915258884429932\n",
      "Epoch 1, batch 414, loss = 5.895052909851074\n",
      "Epoch 1, batch 415, loss = 5.940950870513916\n",
      "Epoch 1, batch 416, loss = 5.944430351257324\n",
      "Epoch 1, batch 417, loss = 5.912314414978027\n",
      "Epoch 1, batch 418, loss = 5.8715596199035645\n",
      "Epoch 1, batch 419, loss = 5.936333179473877\n",
      "Epoch 1, batch 420, loss = 5.923141002655029\n",
      "Epoch 1, batch 421, loss = 5.868526458740234\n",
      "Epoch 1, batch 422, loss = 5.986415386199951\n",
      "Epoch 1, batch 423, loss = 6.053003311157227\n",
      "Epoch 1, batch 424, loss = 5.915310382843018\n",
      "Epoch 1, batch 425, loss = 5.914224147796631\n",
      "Epoch 1, batch 426, loss = 5.920725345611572\n",
      "Epoch 1, batch 427, loss = 5.944288730621338\n",
      "Epoch 1, batch 428, loss = 6.002829551696777\n",
      "Epoch 1, batch 429, loss = 5.863736629486084\n",
      "Epoch 1, batch 430, loss = 5.945252418518066\n",
      "Epoch 1, batch 431, loss = 5.873159408569336\n",
      "Epoch 1, batch 432, loss = 6.030143737792969\n",
      "Epoch 1, batch 433, loss = 5.953062534332275\n",
      "Epoch 1, batch 434, loss = 5.9288105964660645\n",
      "Epoch 1, batch 435, loss = 5.967692852020264\n",
      "Epoch 1, batch 436, loss = 6.018709182739258\n",
      "Epoch 1, batch 437, loss = 5.986886978149414\n",
      "Epoch 1, batch 438, loss = 5.8822245597839355\n",
      "Epoch 1, batch 439, loss = 5.919928550720215\n",
      "Epoch 1, batch 440, loss = 5.9240403175354\n",
      "Epoch 1, batch 441, loss = 6.001293182373047\n",
      "Epoch 1, batch 442, loss = 5.913076877593994\n",
      "Epoch 1, batch 443, loss = 5.92920446395874\n",
      "Epoch 1, batch 444, loss = 5.94022798538208\n",
      "Epoch 1, batch 445, loss = 5.888833522796631\n",
      "Epoch 1, batch 446, loss = 6.002413749694824\n",
      "Epoch 1, batch 447, loss = 5.890798091888428\n",
      "Epoch 1, batch 448, loss = 5.929871082305908\n",
      "Epoch 1, batch 449, loss = 5.839600563049316\n",
      "Epoch 1, batch 450, loss = 5.82992696762085\n",
      "Epoch 1, batch 451, loss = 5.907683372497559\n",
      "Epoch 1, batch 452, loss = 5.997952938079834\n",
      "Epoch 1, batch 453, loss = 6.027944564819336\n"
     ]
    }
   ],
   "source": [
    "chatbot.train(session,\n",
    "              1, 256,\n",
    "              input_sentences_idx,\n",
    "              ground_truth_sentences_idx,\n",
    "              input_lengths,\n",
    "              ground_truth_lengths,\n",
    "              1e-4,\n",
    "              0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i [END]'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.get_reply(session, \"who are you\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
